{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12a42c83",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c429dea",
   "metadata": {},
   "source": [
    "- 어텐션(attention)은 입력에 대한 벡터 변환을 인코더(encoder)에서 처리하고 모든 벡터를 디코더(decoder)로 보냄.\n",
    "이렇게 모든 벡터를 전달하는 것은 시간이 흐를수록 초기 정보를 잃어버리는 `기울기 소실`문제를 해결하기 위함임. \n",
    "그러나 모든 벡터가 전달됨으로써 행렬 크기가 매우 커지는 단점이 있는데, 이를 해결하기 위해 `소프트맥스 함수`를 사용하여\n",
    "가중합을 구하고, 그 값을 디코더에 전달함.\n",
    "\n",
    "- 가중합이 전달되면서, 정보를 많이 전달받은 디코더에게 부담이 가기 때문에, 디코더는 은닉 상태에 대하여\n",
    "중점적으로 `집중(attention)`해 보아야 할 벡터를 소프트맥스 함수로 점수매긴 후 각각을 은닉 상태 벡터들과 곱함.\n",
    "그리고 이 은닉 상태를 모두 더하여 하나의 값으로 만듦. 즉, 어텐션은 모든 벡터 중 꼭 살펴봐야 할 벡터에 집중하겠다는 의미임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53bd24a",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "- 트랜스포머는 어텐션을 극대화하는 방법으로, 인코더와 디코더를 여러개 중첩시킨 구조임.\n",
    "이 때 각각의 인코더와 디코더를 `block`이라고 함(논문에서는 인코더와 디코더 블록을 6개씩 중첩한 구조 사용)\n",
    "\n",
    "- 하나의 인코더는 `self-attention`과 `전방향 신경망(feed-forward neural network)`으로 구성되어 있음.\n",
    "인코더에서는 단어를 벡터로 임베딩하며, 이를 셀프 어텐션과 전방향 신경망으로 전달함.\n",
    "이 때 셀프 어텐션은 문장에서 각 단어끼리 얼마나 관계하는지를 계산해서 반영함. 즉, 셀프 어텐션으로\n",
    "문장 안에서 단어 간 관계를 파악할 수 있음. 셀프 어텐션에서 파악된 단어간 관계는 전방향 신경망으로 전달됨\n",
    "\n",
    "- 디코더는 층을 총 3개 가지는데, 인코더에서 넘어온 벡터가 처음 만나는 것이 self-attention 층임.(인코더와 동일)\n",
    "셀프 어텐션 층을 지나면 인코더-디코더 어텐션 층이 있음. 이 층에서는 인코더가 처리한 정보를 받아 어텐션 메커니즘을 수행하고,\n",
    "마지막으로 전방향 신경망으로 데이터가 전달됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc6aed4",
   "metadata": {},
   "source": [
    "### 어텐션 메커니즘\n",
    "- 어텐션 메커니즘을 이용하기 위해서는 가장 먼저 `어텐션 스코어`를 구해야 함\n",
    "- 어텐션 스코어란, 현 디코더의 시점 i에서 단어를 예측하기 위해, 인코더의 모든 은닉상태 값($h_j$)이\n",
    "디코더의 현 시점 은닉 상태($s_i$)와 얼마나 유사한지(관련이 있는지)를 판단하는 값임. 따라서,\n",
    "어텐션 스코어는 앞 수식처럼 인코더의 모든 은닉 상태 값($h_j$)과 디코더에서 이전 시점 은닉상태($s_{i-1}$)\n",
    "값을 이용하여 구할 수 있음\n",
    "- 어텐션 스코어가 계산되면, 이 값을 소프트맥스 함수에 적용하여 확률로 변환하고,\n",
    "이렇게 계산된 0 ~ 1 사이의 값들이 특정 시점(timestep)에 대한 가중치, 즉 시간의 가중치가 됨.\n",
    "- 시간의 가중치($a_{ij}$)와 은닉 상태($h_j$)의 가중합을 계산하면 하나의 벡터가 계산되는데, 이것이 컨텍스트 벡터(context vector)임.\n",
    "- 마지막으로 디코더의 은닉 상태를 구하는데, 이를 위해 컨텍스트 벡터와 디코더 이전 시점의 은닉 상태와 출력이 필요함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137973bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "89aedc3e",
   "metadata": {},
   "source": [
    "## Seq2seq\n",
    "- `seq2seq(sequence to seqeunce)`는 입력 시퀀스에 대한 출력 시퀀스를 만들기 위한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5947f13d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "\n",
    "import re\n",
    "import random\n",
    "\n",
    "device =torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "287b99df",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 20\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self):\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  \n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c088835f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정규화\n",
    "def normalizeString(df, lang) : \n",
    "    sentence = df[lang].str.lower() # 소문자 전환\n",
    "    sentence = sentence.str.replace('[^A-Za-z\\s]+', ' ')\n",
    "    sentence = sentence.str.normalize('NFD') # 유니코드 정규화\n",
    "    sentence = sentence.str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    return sentence\n",
    "\n",
    "def read_sentence(df, lang1, lang2) : \n",
    "    sentence1 = normalizeString(df, lang1) # 데이터셋 1번째 열 \n",
    "    sentence2 = normalizeString(df, lang2)\n",
    "    return sentence1, sentence2\n",
    "\n",
    "def read_file(loc, lang1, lang2) :\n",
    "    df = pd.read_csv(loc, delimiter='\\t', header=None, names=[lang1, lang2])\n",
    "    return df\n",
    "\n",
    "def process_data(lang1, lang2) :\n",
    "    df = read_file('../080289-main/chap10/data/%s-%s.txt'%(lang1, lang2), lang1, lang2) # load data\n",
    "    sentence1, sentence2 = read_sentence(df, lang1, lang2)\n",
    "    \n",
    "    input_lang = Lang()\n",
    "    output_lang = Lang()\n",
    "    pairs = []\n",
    "    for i in range(len(df)):\n",
    "        if len(sentence1[i].split(' ')) < MAX_LENGTH and len(sentence2[i].split(' ')) < MAX_LENGTH:\n",
    "            full = [sentence1[i], sentence2[i]]\n",
    "            input_lang.addSentence(sentence1[i])\n",
    "            output_lang.addSentence(sentence2[i])\n",
    "            pairs.append(full)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79dde573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor로 변환\n",
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
    "\n",
    "def tensorsFromPair(input_lang, output_lang, pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, target_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d882609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 인코더 네트워크\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embbed_dim, num_layers):\n",
    "        super(Encoder, self).__init__()       \n",
    "        self.input_dim = input_dim\n",
    "        self.embbed_dim = embbed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(input_dim, self.embbed_dim)\n",
    "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
    "              \n",
    "    def forward(self, src):      \n",
    "        embedded = self.embedding(src).view(1,1,-1)\n",
    "        outputs, hidden = self.gru(embedded)\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9500831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 디코더 네트워크 \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim, embbed_dim, num_layers):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.embbed_dim = embbed_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, self.embbed_dim)\n",
    "        self.gru = nn.GRU(self.embbed_dim, self.hidden_dim, num_layers=self.num_layers)\n",
    "        self.out = nn.Linear(self.hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "      \n",
    "    def forward(self, input, hidden):\n",
    "        input = input.view(1, -1)\n",
    "        embedded = F.relu(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded, hidden)       \n",
    "        prediction = self.softmax(self.out(output[0]))      \n",
    "        return prediction, hidden "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbd6f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seq2seq 네트워크\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device, MAX_LENGTH=MAX_LENGTH):\n",
    "        super().__init__()\n",
    "        # 인코더와 디코더 초기화\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "     \n",
    "    def forward(self, input_lang, output_lang, teacher_forcing_ratio=0.5):\n",
    "\n",
    "        input_length = input_lang.size(0) # 입력 문장 길이(문장 단어수)\n",
    "        batch_size = output_lang.shape[1] \n",
    "        target_length = output_lang.shape[0]\n",
    "        vocab_size = self.decoder.output_dim      \n",
    "        outputs = torch.zeros(target_length, batch_size, vocab_size).to(self.device)\n",
    "\n",
    "        for i in range(input_length):\n",
    "            # 문장의 모든 단어 인코딩\n",
    "            encoder_output, encoder_hidden = self.encoder(input_lang[i])\n",
    "            \n",
    "        # 인코더 은닉층 -> 디코더 은닉층\n",
    "        decoder_hidden = encoder_hidden.to(device)  \n",
    "        # 예측 단어 앞에 SOS token 추가\n",
    "        decoder_input = torch.tensor([SOS_token], device=device)  \n",
    "\n",
    "        for t in range(target_length):   \n",
    "            # 현재 단어에서 출력단어 예측\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            outputs[t] = decoder_output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            # teacher force 활성화하면 모표를 다음 입력으로 사용\n",
    "            input = (output_lang[t] if teacher_force else topi)\n",
    "            # teacher force 활성화하지 않으면 자체 예측 값을 다음 입력으로 사용\n",
    "            if (teacher_force == False and input.item() == EOS_token) :\n",
    "                break\n",
    "        return outputs\n",
    "    \n",
    "# teacher_force : seq2seq에서 많이 사용되는 기법. 번역(예측)하려는 모표 단어를 디코더의 다음 입력으로 넣어줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5fcdc0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 오차 계산 함수 정의\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def Model(model, input_tensor, target_tensor, model_optimizer, criterion):\n",
    "    model_optimizer.zero_grad()\n",
    "    input_length = input_tensor.size(0)\n",
    "    loss = 0\n",
    "    epoch_loss = 0\n",
    "    output = model(input_tensor, target_tensor)\n",
    "    num_iter = output.size(0)\n",
    "\n",
    "    for ot in range(num_iter):\n",
    "        loss += criterion(output[ot], target_tensor[ot])\n",
    "\n",
    "    loss.backward()\n",
    "    model_optimizer.step()\n",
    "    epoch_loss = loss.item() / num_iter\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70fd0324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련함수 정의\n",
    "def trainModel(model, input_lang, output_lang, pairs, num_iteration=20000):\n",
    "    model.train()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss_iterations = 0\n",
    "\n",
    "    training_pairs = [tensorsFromPair(input_lang, output_lang, random.choice(pairs))\n",
    "                      for i in range(num_iteration)]\n",
    "  \n",
    "    for iter in range(1, num_iteration+1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        loss = Model(model, input_tensor, target_tensor, optimizer, criterion)\n",
    "        total_loss_iterations += loss\n",
    "\n",
    "        if iter % 5000 == 0:\n",
    "            average_loss= total_loss_iterations / 5000\n",
    "            total_loss_iterations = 0\n",
    "            print('%d %.4f' % (iter, average_loss))\n",
    "          \n",
    "    torch.save(model.state_dict(), './mytraining.pt')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "820380da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 평가\n",
    "def evaluate(model, input_lang, output_lang, sentences, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentences[0])\n",
    "        output_tensor = tensorFromSentence(output_lang, sentences[1])  \n",
    "        decoded_words = []  \n",
    "        output = model(input_tensor, output_tensor)\n",
    "  \n",
    "        for ot in range(output.size(0)):\n",
    "            topv, topi = output[ot].topk(1)\n",
    "\n",
    "            if topi[0].item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi[0].item()])\n",
    "    return decoded_words\n",
    "\n",
    "def evaluateRandomly(model, input_lang, output_lang, pairs, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('input {}'.format(pair[0]))\n",
    "        print('output {}'.format(pair[1]))\n",
    "        output_words = evaluate(model, input_lang, output_lang, pair)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('predicted {}'.format(output_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68de7eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\envs\\yeseul\\lib\\site-packages\\ipykernel_launcher.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random sentence ['great weather  isn t it ', 'temps magnifique  n est ce pas ']\n",
      "Input : 12671 Output : 18454\n",
      "Encoder(\n",
      "  (embedding): Embedding(12671, 256)\n",
      "  (gru): GRU(256, 512)\n",
      ")\n",
      "Decoder(\n",
      "  (embedding): Embedding(18454, 256)\n",
      "  (gru): GRU(256, 512)\n",
      "  (out): Linear(in_features=512, out_features=18454, bias=True)\n",
      "  (softmax): LogSoftmax(dim=1)\n",
      ")\n",
      "5000 4.8856\n",
      "10000 4.8447\n",
      "15000 4.8432\n",
      "20000 4.8344\n",
      "25000 4.8230\n",
      "30000 4.8251\n",
      "35000 4.8005\n",
      "40000 4.8146\n",
      "45000 4.8014\n",
      "50000 4.7815\n",
      "55000 4.8139\n",
      "60000 4.8271\n",
      "65000 4.8388\n",
      "70000 4.8086\n",
      "75000 4.8200\n"
     ]
    }
   ],
   "source": [
    "lang1 = 'eng'\n",
    "lang2 = 'fra'\n",
    "input_lang, output_lang, pairs = process_data(lang1, lang2)\n",
    "\n",
    "randomize = random.choice(pairs)\n",
    "print('random sentence {}'.format(randomize))\n",
    "\n",
    "input_size = input_lang.n_words\n",
    "output_size = output_lang.n_words\n",
    "print('Input : {} Output : {}'.format(input_size, output_size))\n",
    "\n",
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "num_iteration = 75000\n",
    "\n",
    "encoder = Encoder(input_size, hidden_size, embed_size, num_layers)\n",
    "decoder = Decoder(output_size, hidden_size, embed_size, num_layers)\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    " \n",
    "print(encoder)\n",
    "print(decoder)\n",
    "\n",
    "model = trainModel(model, input_lang, output_lang, pairs, num_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4dff63e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input have you ever eaten anything that made you hallucinate \n",
      "output as tu jamais mang  quoi que ce soit qui t ait fait halluciner  \n",
      "predicted je est a pas       <EOS>\n",
      "input do you want to go to the station with me \n",
      "output voulez vous venir  avec moi  jusqu  la gare  \n",
      "predicted je est a pas       <EOS>\n",
      "input i m surprised you didn t know that \n",
      "output je suis surpris que tu ignorais cela \n",
      "predicted je est a pas     \n",
      "input he asked me for help \n",
      "output il m a demand  de l aide \n",
      "predicted je est a pas      \n",
      "input i m not afraid of you \n",
      "output je n ai pas peur de vous \n",
      "predicted je est a pas     \n",
      "input how would you like us to proceed \n",
      "output comment voudrais tu que nous proc dions  \n",
      "predicted je est a pas      \n",
      "input my father quit drinking \n",
      "output mon p re a arr t  de boire \n",
      "predicted je est a pas       <EOS>\n",
      "input i hate this town \n",
      "output je d teste cette ville \n",
      "predicted je est a pas   \n",
      "input he is worthy of our praise \n",
      "output il m rite nos louanges \n",
      "predicted je est a pas   \n",
      "input don t walk on the grass \n",
      "output ne marchez pas sur la pelouse \n",
      "predicted je est a pas    \n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(model, input_lang, output_lang, pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e48844",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8433ce4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yeseul",
   "language": "python",
   "name": "yeseul"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
